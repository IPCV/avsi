---
layout: page
<!--gh-repo: vskadandale/vocalist-->
<!--gh-badge: [star, watch, fork, follow]-->
share-description: Official website of Speech Inpainting: Context-based Speech Synthesis Guided by Video
---
<div class="container">
    <div class="row">
        <div class="col-xl-12 mx-auto text-center">
            <h1>Speech Inpainting: Context-based Speech Synthesis Guided by Video</h1>
        </div>
        <div class="col-md-10 col-lg-8 col-xl-7 mx-auto">
        </div>
    </div>
</div>


<div class="col-xl-10 col-lg-8 offset-lg-1">

    <!-- Testimonials -->
    <section class="testimonials text-center">
        <div class="container">
            <div class="row">
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:juanfelipe.montesinosATupfDOTedu"
                               style="text-decoration : none; color : #000000;">
                                Juan F. Montesinos
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:dnmhATdemantDOTcom" style="text-decoration : none; color : #000000;">
                                Daniel Michelsanti
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:gloria.haroATupfDOTedu" style="text-decoration : none; color : #000000;">
                                Gloria Haro
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:ztATesDOTaauDOTdk" style="text-decoration : none; color : #000000;">
                                Zheng-Hua Tan
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
                <div class="col-lg-4 text-center">
                    <div class="testimonial-item mx-auto mb-5 mb-lg-0">
                        <h5>
                            <a href="mailto:jesjATdemantDOTcom" style="text-decoration : none; color : #000000;">
                                Jesper Jensen
                            </a>
                        </h5>
                        <p class="font-weight-light mb-0"></p>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="offset-lg-3 col-lg-6 padtop" style="padding-bottom: 2rem">
          <span class="align-middle">
            <p class="mylead2">
                <a href="https://www.upf.edu/web/etic"
                   style="color:black">Universitat Pompeu Fabra, Barcelona, Spain</a><br>

              
          </span>
                </div>
            </div>
        </div>
    </section>
    <div class="row justify-content-center">
        <div class="col-sm-3 text-center">
            <a target="_blank"
               href=""><img src="assets/img/paper_thumbnail.png" width="120"
                            height="130" style="border:1px solid black;"></a>
            <h5 style="padding-bottom: 5%; padding-top: 5%">Paper</h5>
        </div>
        <div class="col-sm-3 text-center">
            <a href=""
               style="color: #242124">
                <i class="fab fa-github fa-8x"></i></a>
            <h5 style="padding-bottom: 5%; padding-top: 5%">Code + Weights</h5>
        </div>
        <div class="col-sm-3 text-center">
            <a style="color: #242124;"
               href="./demos/">
                <i class="fas fa-film fa-8x" style="transform: scale(1,1.275); padding-top: 2.5px"></i>
            </a>
            <h5 style="padding-bottom: 5%; padding-top: 5px">Demos</h5>
        </div>
    </div>
    <h6 class="mx-auto text-center" style="color: saddlebrown">Paper under review</h6>
    </br>

    </br>
    </br>

    <!-- Image Showcases -->
    <h2 style="text-align: center">Abstract</h2>
    <p class="lead mb-0" align="justify">
        Audio and visual modalities are inherently connected in
        speech signals: Lips movements and facial expressions are
        correlated with the production of speech sounds. This moti-
        vates the studies that incorporate visual modality to enhance
        an acoustic speech signal or even restore missing audio in-
        formation. In this paper, we present a transformer-based
        deep learning model which produces state-of-the-art results
        in audio-visual speech inpainting. Given an audio-visual sig-
        nal whose audio stream is partially corrupted, audio-visual
        speech inpainting is the task of synthesizing the audio of
        the corrupted segment coherently to the corresponding video
        and the uncorrupted audio. We compare the performance
        of our model against the previous state-of-the-art model and
        audio-only baselines, showing the importance of having an
        additional cue that provides information about the content of
        the corrupted audio. We also show how the visual features
        from AV-Hubert [1] are suitable for synthesizing speech.
    </p>
    </br>

    <!--    <div class="mx-auto">-->
    <!--        <br>-->
    <!--        <h5>Citation</h5>-->
    <!--        <pre class="hightlight" style="background-color:rgba(0,0,0, 0.1)"><p class="mb-0" align="justify">@inproceedings{kadandale2022vocalist,-->
    <!--  title={VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices},-->
    <!--  author={Kadandale, Venkatesh S and Montesinos, Juan F and Haro, Gloria},-->
    <!--  booktitle={Interspeech},-->
    <!--  pages={3128&#45;&#45;3132},-->
    <!--  year={2022}-->
    <!--}</p></pre>-->
    <!--    </div>-->

    <div class="mx-auto">
        <br>
        <h5>Acknowledgements</h5>
        <p class="lead mb-0" align="justify">
            The authors acknowledge support by MICINN/FEDER UE project, ref. PGC2018-098625-B-I00; PID2021-127643NB-I00
            project;
            H2020-MSCA-RISE-2017 project, ref. 777826 NoMADS; ReAViPeRo network, ref. RED2018-102511-T; and Spanish
            Ministry of Economy and Competitiveness under the Mar√≠a de Maeztu Units of Excellence Program
            (MDM-2015-0502) and the Social European Funds. J. F. M. acknowledges support by FPI scholarship
            PRE2018-083920 We gratefully acknowledge NVIDIA Corporation for the donation of GPUs used for the
            experiments.
        </p>
    </div>

</div>
